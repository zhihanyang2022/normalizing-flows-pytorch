{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4adc30a8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce0f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808d310d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhihanyang/opt/miniconda3/envs/mlp/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: dlopen(/Users/zhihanyang/opt/miniconda3/envs/mlp/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libpng16.16.dylib\n",
      "  Referenced from: <8E4D9E61-A0A3-30A7-B778-23E3E702B690> /Users/zhihanyang/opt/miniconda3/envs/mlp/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Users/zhihanyang/opt/miniconda3/envs/mlp/lib/libpng16.16.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/zhihanyang/opt/miniconda3/envs/mlp/lib/libpng16.16.dylib' (no such file), '/Users/zhihanyang/opt/miniconda3/envs/mlp/lib/libpng16.16.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/zhihanyang/opt/miniconda3/envs/mlp/lib/libpng16.16.dylib' (no such file), '/Users/zhihanyang/opt/miniconda3/envs/mlp/lib/python3.8/lib-dynload/../../libpng16.16.dylib' (no such file), '/Users/zhihanyang/opt/miniconda3/envs/mlp/lib/libpng16.16.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/zhihanyang/opt/miniconda3/envs/mlp/lib/libpng16.16.dylib' (no such file), '/Users/zhihanyang/opt/miniconda3/envs/mlp/bin/../lib/libpng16.16.dylib' (no such file), '/usr/local/lib/libpng16.16.dylib' (no such file), '/usr/lib/libpng16.16.dylib' (no such file, not in dyld cache)\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as d\n",
    "import torch.optim as optim\n",
    "\n",
    "from core_vectorized import ConditionalNF\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets.mnist import load_data\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "import torchvision\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd91ee",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cabbf678",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim, x_dim):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.x_dim = x_dim\n",
    "        self.mus = nn.Sequential(\n",
    "            nn.Linear(z_dim, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, x_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, zs):\n",
    "        return d.Independent(d.Bernoulli(logits=self.mus(zs)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c468c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = Decoder(z_dim=40, x_dim=784)\n",
    "dec(torch.randn(64, 10, 40)).log_prob(torch.zeros(64, 1, 784)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e574464a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_z = d.Independent(d.Normal(torch.zeros(2), torch.ones(2)), 1)\n",
    "p_z.log_prob(torch.randn(32, 10, 2)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764aa026",
   "metadata": {},
   "source": [
    "## NFVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2c515ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFVAE:\n",
    "\n",
    "    def __init__(self, z_dim, x_dim, lr, num_flows, num_samples_per_x=1):\n",
    "\n",
    "        # hyper-parameters\n",
    "        self.z_dim = z_dim\n",
    "        self.x_dim = x_dim\n",
    "        self.lr = lr\n",
    "        self.num_flows = num_flows\n",
    "        self.num_samples_per_x = num_samples_per_x\n",
    "\n",
    "        # describes the generative process\n",
    "        self.p_z = d.Independent(d.Normal(torch.zeros(z_dim), torch.ones(z_dim)), 1)\n",
    "        self.p_x_given_z = Decoder(z_dim=z_dim, x_dim=x_dim)\n",
    "\n",
    "        # required for approximate posterior inference\n",
    "        self.q_z_given_x = ConditionalNF(x_dim=x_dim, D=z_dim, L=self.num_flows)\n",
    "\n",
    "        # gradient-based optimizers\n",
    "        self.p_x_given_z_opt = optim.Adam(self.p_x_given_z.parameters(), lr=lr)\n",
    "        self.q_z_given_x_opt = optim.Adam(self.q_z_given_x.parameters(), lr=lr)\n",
    "\n",
    "    def fit(self, xs):\n",
    "        self.p_x_given_z.train()\n",
    "        self.q_z_given_x.train()\n",
    "        zs, log_probs = self.q_z_given_x.rsample(xs, num_samples_per_x=self.num_samples_per_x)  # (bs, 10, z_dim), (bs, 10)\n",
    "        # kl-divergence\n",
    "        kl = (log_probs - self.p_z.log_prob(zs)).mean()  # ok this works\n",
    "        # reconstruction\n",
    "        rec = self.p_x_given_z(zs).log_prob(xs.reshape(xs.shape[0], 1, xs.shape[1])).mean()  # ok this works\n",
    "        # elbo (estimated using one sample per posterior)\n",
    "        elbo = - kl + rec\n",
    "        # backpropagation and gradient step\n",
    "        loss = - elbo\n",
    "        self.p_x_given_z_opt.zero_grad()\n",
    "        self.q_z_given_x_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.p_x_given_z_opt.step()\n",
    "        self.q_z_given_x_opt.step()\n",
    "        return {\n",
    "            \"kl\": float(kl),  # this is at least 0; the smaller the better\n",
    "            \"rec\": float(rec),  # the larger the better\n",
    "            \"elbo\": float(elbo)  # the larger the better\n",
    "        }\n",
    "\n",
    "    def encode(self, xs):\n",
    "        with torch.no_grad():\n",
    "            return self.q_z_given_x(xs).mean\n",
    "\n",
    "    def sample(self, n):\n",
    "        self.p_x_given_z.eval()\n",
    "        self.q_z_given_x.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.p_x_given_z(self.p_z.sample((n, ))).mean  # or .sample()\n",
    "\n",
    "    def save(self, save_dir):\n",
    "        torch.save(self.p_x_given_z.state_dict(), os.path.join(save_dir, \"p_x_given_z.pth\"))\n",
    "        torch.save(self.q_z_given_x.state_dict(), os.path.join(save_dir, \"q_z_given_x.pth\"))\n",
    "\n",
    "    def load(self, save_dir):\n",
    "        self.p_x_given_z.load_state_dict(\n",
    "            torch.load(os.path.join(save_dir, \"p_x_given_z.pth\"), map_location=torch.device(\"cpu\"))\n",
    "        )\n",
    "        self.q_z_given_x.load_state_dict(\n",
    "            torch.load(os.path.join(save_dir, \"q_z_given_x.pth\"), map_location=torch.device(\"cpu\"))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffdcc93",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf07c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_train[x_train >= 0.5] = 1\n",
    "x_train[x_train < 0.5] = 0\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "\n",
    "x_test = x_test / 255\n",
    "x_test[x_test >= 0.5] = 1\n",
    "x_test[x_test < 0.5] = 0\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(x_train).float())\n",
    "train_dl = DataLoader(train_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea912654",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02408df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Seed 1\n",
      "Epoch 2 | Seed 1\n",
      "Epoch 3 | Seed 1\n",
      "Epoch 4 | Seed 1\n",
      "Epoch 5 | Seed 1\n",
      "Epoch 6 | Seed 1\n",
      "Epoch 7 | Seed 1\n",
      "Epoch 8 | Seed 1\n",
      "Epoch 9 | Seed 1\n",
      "Epoch 10 | Seed 1\n",
      "Epoch 11 | Seed 1\n",
      "Epoch 12 | Seed 1\n",
      "Epoch 13 | Seed 1\n",
      "Epoch 14 | Seed 1\n",
      "Epoch 15 | Seed 1\n",
      "Epoch 16 | Seed 1\n",
      "Epoch 17 | Seed 1\n",
      "Epoch 18 | Seed 1\n",
      "Epoch 19 | Seed 1\n",
      "Epoch 20 | Seed 1\n",
      "Epoch 21 | Seed 1\n",
      "Epoch 22 | Seed 1\n",
      "Epoch 23 | Seed 1\n",
      "Epoch 24 | Seed 1\n",
      "Epoch 25 | Seed 1\n",
      "Epoch 26 | Seed 1\n",
      "Epoch 27 | Seed 1\n",
      "Epoch 28 | Seed 1\n",
      "Epoch 29 | Seed 1\n",
      "Epoch 30 | Seed 1\n",
      "Epoch 31 | Seed 1\n",
      "Epoch 32 | Seed 1\n",
      "Epoch 33 | Seed 1\n",
      "Epoch 34 | Seed 1\n",
      "Epoch 35 | Seed 1\n",
      "Epoch 36 | Seed 1\n",
      "Epoch 37 | Seed 1\n",
      "Epoch 38 | Seed 1\n",
      "Epoch 39 | Seed 1\n",
      "Epoch 40 | Seed 1\n",
      "Epoch 41 | Seed 1\n",
      "Epoch 42 | Seed 1\n",
      "Epoch 43 | Seed 1\n",
      "Epoch 44 | Seed 1\n",
      "Epoch 45 | Seed 1\n",
      "Epoch 46 | Seed 1\n",
      "Epoch 47 | Seed 1\n",
      "Epoch 48 | Seed 1\n",
      "Epoch 49 | Seed 1\n",
      "Epoch 50 | Seed 1\n",
      "Epoch 51 | Seed 1\n",
      "Epoch 52 | Seed 1\n",
      "Epoch 53 | Seed 1\n",
      "Epoch 54 | Seed 1\n",
      "Epoch 55 | Seed 1\n",
      "Epoch 56 | Seed 1\n",
      "Epoch 57 | Seed 1\n",
      "Epoch 58 | Seed 1\n",
      "Epoch 59 | Seed 1\n",
      "Epoch 60 | Seed 1\n",
      "Epoch 61 | Seed 1\n",
      "Epoch 62 | Seed 1\n",
      "Epoch 63 | Seed 1\n",
      "Epoch 64 | Seed 1\n",
      "Epoch 65 | Seed 1\n",
      "Epoch 66 | Seed 1\n",
      "Epoch 67 | Seed 1\n",
      "Epoch 68 | Seed 1\n",
      "Epoch 69 | Seed 1\n",
      "Epoch 70 | Seed 1\n",
      "Epoch 71 | Seed 1\n",
      "Epoch 72 | Seed 1\n",
      "Epoch 73 | Seed 1\n",
      "Epoch 74 | Seed 1\n",
      "Epoch 75 | Seed 1\n",
      "Epoch 76 | Seed 1\n",
      "Epoch 77 | Seed 1\n",
      "Epoch 78 | Seed 1\n",
      "Epoch 79 | Seed 1\n",
      "Epoch 80 | Seed 1\n",
      "Epoch 81 | Seed 1\n",
      "Epoch 82 | Seed 1\n",
      "Epoch 83 | Seed 1\n",
      "Epoch 84 | Seed 1\n",
      "Epoch 85 | Seed 1\n",
      "Epoch 86 | Seed 1\n",
      "Epoch 87 | Seed 1\n",
      "Epoch 88 | Seed 1\n",
      "Epoch 89 | Seed 1\n",
      "Epoch 90 | Seed 1\n",
      "Epoch 91 | Seed 1\n",
      "Epoch 92 | Seed 1\n",
      "Epoch 93 | Seed 1\n",
      "Epoch 94 | Seed 1\n",
      "Epoch 95 | Seed 1\n",
      "Epoch 96 | Seed 1\n",
      "Epoch 97 | Seed 1\n",
      "Epoch 98 | Seed 1\n",
      "Epoch 99 | Seed 1\n",
      "Epoch 100 | Seed 1\n",
      "Epoch 1 | Seed 2\n",
      "Epoch 2 | Seed 2\n",
      "Epoch 3 | Seed 2\n",
      "Epoch 4 | Seed 2\n",
      "Epoch 5 | Seed 2\n",
      "Epoch 6 | Seed 2\n",
      "Epoch 7 | Seed 2\n",
      "Epoch 8 | Seed 2\n",
      "Epoch 9 | Seed 2\n",
      "Epoch 10 | Seed 2\n",
      "Epoch 11 | Seed 2\n",
      "Epoch 12 | Seed 2\n",
      "Epoch 13 | Seed 2\n",
      "Epoch 14 | Seed 2\n",
      "Epoch 15 | Seed 2\n",
      "Epoch 16 | Seed 2\n",
      "Epoch 17 | Seed 2\n",
      "Epoch 18 | Seed 2\n",
      "Epoch 19 | Seed 2\n",
      "Epoch 20 | Seed 2\n",
      "Epoch 21 | Seed 2\n",
      "Epoch 22 | Seed 2\n",
      "Epoch 23 | Seed 2\n",
      "Epoch 24 | Seed 2\n",
      "Epoch 25 | Seed 2\n",
      "Epoch 26 | Seed 2\n",
      "Epoch 27 | Seed 2\n",
      "Epoch 28 | Seed 2\n",
      "Epoch 29 | Seed 2\n",
      "Epoch 30 | Seed 2\n",
      "Epoch 31 | Seed 2\n",
      "Epoch 32 | Seed 2\n",
      "Epoch 33 | Seed 2\n",
      "Epoch 34 | Seed 2\n",
      "Epoch 35 | Seed 2\n",
      "Epoch 36 | Seed 2\n",
      "Epoch 37 | Seed 2\n",
      "Epoch 38 | Seed 2\n",
      "Epoch 39 | Seed 2\n",
      "Epoch 40 | Seed 2\n",
      "Epoch 41 | Seed 2\n",
      "Epoch 42 | Seed 2\n",
      "Epoch 43 | Seed 2\n",
      "Epoch 44 | Seed 2\n",
      "Epoch 45 | Seed 2\n",
      "Epoch 46 | Seed 2\n",
      "Epoch 47 | Seed 2\n",
      "Epoch 48 | Seed 2\n",
      "Epoch 49 | Seed 2\n",
      "Epoch 50 | Seed 2\n",
      "Epoch 51 | Seed 2\n",
      "Epoch 52 | Seed 2\n",
      "Epoch 53 | Seed 2\n",
      "Epoch 54 | Seed 2\n",
      "Epoch 55 | Seed 2\n",
      "Epoch 56 | Seed 2\n",
      "Epoch 57 | Seed 2\n",
      "Epoch 58 | Seed 2\n",
      "Epoch 59 | Seed 2\n",
      "Epoch 60 | Seed 2\n",
      "Epoch 61 | Seed 2\n",
      "Epoch 62 | Seed 2\n",
      "Epoch 63 | Seed 2\n",
      "Epoch 64 | Seed 2\n",
      "Epoch 65 | Seed 2\n",
      "Epoch 66 | Seed 2\n",
      "Epoch 67 | Seed 2\n",
      "Epoch 68 | Seed 2\n",
      "Epoch 69 | Seed 2\n",
      "Epoch 70 | Seed 2\n",
      "Epoch 71 | Seed 2\n",
      "Epoch 72 | Seed 2\n",
      "Epoch 73 | Seed 2\n",
      "Epoch 74 | Seed 2\n",
      "Epoch 75 | Seed 2\n",
      "Epoch 76 | Seed 2\n",
      "Epoch 77 | Seed 2\n",
      "Epoch 78 | Seed 2\n",
      "Epoch 79 | Seed 2\n",
      "Epoch 80 | Seed 2\n",
      "Epoch 81 | Seed 2\n",
      "Epoch 82 | Seed 2\n",
      "Epoch 83 | Seed 2\n",
      "Epoch 84 | Seed 2\n",
      "Epoch 85 | Seed 2\n",
      "Epoch 86 | Seed 2\n",
      "Epoch 87 | Seed 2\n",
      "Epoch 88 | Seed 2\n",
      "Epoch 89 | Seed 2\n",
      "Epoch 90 | Seed 2\n",
      "Epoch 91 | Seed 2\n",
      "Epoch 92 | Seed 2\n",
      "Epoch 93 | Seed 2\n",
      "Epoch 94 | Seed 2\n",
      "Epoch 95 | Seed 2\n",
      "Epoch 96 | Seed 2\n",
      "Epoch 97 | Seed 2\n",
      "Epoch 98 | Seed 2\n",
      "Epoch 99 | Seed 2\n",
      "Epoch 100 | Seed 2\n",
      "Epoch 1 | Seed 3\n",
      "Epoch 2 | Seed 3\n",
      "Epoch 3 | Seed 3\n",
      "Epoch 4 | Seed 3\n",
      "Epoch 5 | Seed 3\n",
      "Epoch 6 | Seed 3\n",
      "Epoch 7 | Seed 3\n",
      "Epoch 8 | Seed 3\n",
      "Epoch 9 | Seed 3\n",
      "Epoch 10 | Seed 3\n",
      "Epoch 11 | Seed 3\n",
      "Epoch 12 | Seed 3\n",
      "Epoch 13 | Seed 3\n",
      "Epoch 14 | Seed 3\n",
      "Epoch 15 | Seed 3\n",
      "Epoch 16 | Seed 3\n",
      "Epoch 17 | Seed 3\n",
      "Epoch 18 | Seed 3\n",
      "Epoch 19 | Seed 3\n",
      "Epoch 20 | Seed 3\n",
      "Epoch 21 | Seed 3\n",
      "Epoch 22 | Seed 3\n",
      "Epoch 23 | Seed 3\n",
      "Epoch 24 | Seed 3\n",
      "Epoch 25 | Seed 3\n",
      "Epoch 26 | Seed 3\n",
      "Epoch 27 | Seed 3\n",
      "Epoch 28 | Seed 3\n",
      "Epoch 29 | Seed 3\n",
      "Epoch 30 | Seed 3\n",
      "Epoch 31 | Seed 3\n",
      "Epoch 32 | Seed 3\n",
      "Epoch 33 | Seed 3\n",
      "Epoch 34 | Seed 3\n",
      "Epoch 35 | Seed 3\n",
      "Epoch 36 | Seed 3\n",
      "Epoch 37 | Seed 3\n",
      "Epoch 38 | Seed 3\n",
      "Epoch 39 | Seed 3\n",
      "Epoch 40 | Seed 3\n",
      "Epoch 41 | Seed 3\n",
      "Epoch 42 | Seed 3\n",
      "Epoch 43 | Seed 3\n",
      "Epoch 44 | Seed 3\n",
      "Epoch 45 | Seed 3\n",
      "Epoch 46 | Seed 3\n",
      "Epoch 47 | Seed 3\n",
      "Epoch 48 | Seed 3\n",
      "Epoch 49 | Seed 3\n",
      "Epoch 50 | Seed 3\n",
      "Epoch 51 | Seed 3\n",
      "Epoch 52 | Seed 3\n",
      "Epoch 53 | Seed 3\n",
      "Epoch 54 | Seed 3\n",
      "Epoch 55 | Seed 3\n",
      "Epoch 56 | Seed 3\n",
      "Epoch 57 | Seed 3\n",
      "Epoch 58 | Seed 3\n",
      "Epoch 59 | Seed 3\n",
      "Epoch 60 | Seed 3\n",
      "Epoch 61 | Seed 3\n",
      "Epoch 62 | Seed 3\n",
      "Epoch 63 | Seed 3\n",
      "Epoch 64 | Seed 3\n",
      "Epoch 65 | Seed 3\n",
      "Epoch 66 | Seed 3\n",
      "Epoch 67 | Seed 3\n",
      "Epoch 68 | Seed 3\n",
      "Epoch 69 | Seed 3\n",
      "Epoch 70 | Seed 3\n",
      "Epoch 71 | Seed 3\n",
      "Epoch 72 | Seed 3\n",
      "Epoch 73 | Seed 3\n",
      "Epoch 74 | Seed 3\n",
      "Epoch 75 | Seed 3\n",
      "Epoch 76 | Seed 3\n",
      "Epoch 77 | Seed 3\n",
      "Epoch 78 | Seed 3\n",
      "Epoch 79 | Seed 3\n",
      "Epoch 80 | Seed 3\n",
      "Epoch 81 | Seed 3\n",
      "Epoch 82 | Seed 3\n",
      "Epoch 83 | Seed 3\n",
      "Epoch 84 | Seed 3\n",
      "Epoch 85 | Seed 3\n",
      "Epoch 86 | Seed 3\n",
      "Epoch 87 | Seed 3\n",
      "Epoch 88 | Seed 3\n",
      "Epoch 89 | Seed 3\n",
      "Epoch 90 | Seed 3\n",
      "Epoch 91 | Seed 3\n",
      "Epoch 92 | Seed 3\n",
      "Epoch 93 | Seed 3\n",
      "Epoch 94 | Seed 3\n",
      "Epoch 95 | Seed 3\n",
      "Epoch 96 | Seed 3\n",
      "Epoch 97 | Seed 3\n",
      "Epoch 98 | Seed 3\n",
      "Epoch 99 | Seed 3\n",
      "Epoch 100 | Seed 3\n"
     ]
    }
   ],
   "source": [
    "num_flows = 16\n",
    "num_samples_per_x = 1\n",
    "\n",
    "for seed in [1, 2, 3]:\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = NFVAE(z_dim=32, x_dim=28*28, lr=1e-4, num_flows=num_flows, num_samples_per_x=num_samples_per_x)\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    elbos_per_epoch = []\n",
    "\n",
    "    for epoch in range(100):\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Seed {seed}\")\n",
    "\n",
    "        elbos, kls, recs = [], [], []\n",
    "\n",
    "        for i, xb in enumerate(train_dl):\n",
    "\n",
    "            stats_dict = model.fit(xb[0])\n",
    "            elbos.append(stats_dict[\"elbo\"])\n",
    "            kls.append(stats_dict[\"kl\"])\n",
    "            recs.append(stats_dict[\"rec\"])\n",
    "\n",
    "        elbos_per_epoch.append(np.mean(elbos))\n",
    "\n",
    "        writer.add_scalar(\"Stat/elbo\", np.mean(elbos), epoch)\n",
    "        writer.add_scalar(\"Stat/kl\", np.mean(kls), epoch)\n",
    "        writer.add_scalar(\"Stat/rec\", np.mean(recs), epoch)\n",
    "\n",
    "        samples = model.sample(n=100).reshape(-1, 1, 28, 28)\n",
    "        grid = torchvision.utils.make_grid(samples, nrow=10)\n",
    "        writer.add_image('Viz/samples', grid, epoch)\n",
    "\n",
    "    save_path = f\"./trained_models/{num_flows}_flows_{num_samples_per_x}_samples_{seed}_seed\"\n",
    "\n",
    "    model.save(save_dir=save_path)\n",
    "\n",
    "    with open(f\"{save_path}/elbos\", \"wb\") as fp:\n",
    "        pickle.dump(elbos_per_epoch, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842d841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
